{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5eaa365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import  KafkaProducer\n",
    "import sys, os, json\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_json, struct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51b6258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "728546f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'quickstart-events4'\n",
    "bootstrap_server=\"localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aec9cacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17722f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adee7bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing args: --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.apache.kafka:kafka-clients:3.5.1 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "scala_version = '2.12'  # TODO: Ensure this is correct\n",
    "spark_version = '3.3.0'\n",
    "\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.5.1'\n",
    "]\n",
    "\n",
    "args = os.environ.get('PYSPARK_SUBMIT_ARGS', '')\n",
    "if not args:\n",
    "    args = f'--packages {\",\".join(packages)}'\n",
    "    print('Using packages', packages)\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = f'{args} pyspark-shell'\n",
    "else:\n",
    "    print(f'Found existing args: {args}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0ba2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Java home\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk1.8.0_251\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d65c89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"ETLPipeline\") \\\n",
    "    .setMaster(\"local\") \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"c:/pyspark/*\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "etl = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f333a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid=\"chaithanya\"\n",
    "pwd=\"12345678\"\n",
    "\n",
    "#sql db details\n",
    "server = \"localhost\"\n",
    "src_db = \"prac\"\n",
    "src_driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98c257b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_url = f\"jdbc:sqlserver://{server}:1433;databaseName={src_db};user={uid};password={pwd};\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b28adf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"select emp_id,emp_name,salary,manager_id from prac.dbo.emp_manager\"\"\"\n",
    "table=\"emp_manager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "490c2d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+----------+\n",
      "|emp_id|emp_name|salary|manager_id|\n",
      "+------+--------+------+----------+\n",
      "|     1|   Ankit| 10000|         4|\n",
      "|     2|   Mohit| 15000|         5|\n",
      "|     3|   Vikas| 10000|         4|\n",
      "|     4|   Rohit|  5000|         2|\n",
      "|     5|   Mudit| 12000|         6|\n",
      "|     6|    Agam| 12000|         2|\n",
      "|     7|  Sanjay|  9000|         2|\n",
      "|     8|  Ashish|  5000|         2|\n",
      "+------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs=etl.read. \\\n",
    "    format(\"jdbc\"). \\\n",
    "    options(driver=src_driver, user=uid, password=pwd, url=src_url,query=query). \\\n",
    "    load()\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "817bc373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while connecting Kafka Unrecognized configs: {'bootstrap_server': ['localhost:9092']}\n",
      "Unrecognized configs: {'bootstrap_server': ['localhost:9092']}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m kafka_producer \u001b[38;5;241m=\u001b[39m connect_kafka_producer()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#  iterate over the alerts csv file and send alerts as the value, and keys can be the alert uuid\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m publish_message(kafka_producer, topic_name, \u001b[43mdfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m())\n",
      "File \u001b[1;32mC:\\Sparkbinaryfiles\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:1988\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1978\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[0;32m   1979\u001b[0m \n\u001b[0;32m   1980\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[0;32m   1986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m-> 1988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[0;32m   1990\u001b[0m     )\n\u001b[0;32m   1991\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6a3ae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|key|               value|\n",
      "+---+--------------------+\n",
      "|  1|{\"emp_id\":1,\"emp_...|\n",
      "|  2|{\"emp_id\":2,\"emp_...|\n",
      "|  3|{\"emp_id\":3,\"emp_...|\n",
      "|  4|{\"emp_id\":4,\"emp_...|\n",
      "|  5|{\"emp_id\":5,\"emp_...|\n",
      "|  6|{\"emp_id\":6,\"emp_...|\n",
      "|  7|{\"emp_id\":7,\"emp_...|\n",
      "|  8|{\"emp_id\":8,\"emp_...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df = dfs.selectExpr(\"CAST(emp_id AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "kafka_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "acf6af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to Kafka topic 'Start'\n"
     ]
    }
   ],
   "source": [
    "kafka_df \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", bootstrap_server) \\\n",
    "    .option(\"topic\", topic_name) \\\n",
    "    .save()\n",
    "print(\"Data written to Kafka topic 'Start'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017aa30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
